1. Why is data cleaning important in real-time data processing?
Accuracy & Reliability → Removes missing, duplicate, or incorrect records so downstream systems use correct data.
Consistency → Standardizes formats (like dates, currencies, text cases), which avoids errors in reports and ML models.
Efficiency → Prevents wasted computation on invalid data and reduces storage/processing costs.
Better Decisions → Real-time analytics dashboards and predictions become more trustworthy.
Compliance & Governance → Ensures the data meets organizational and regulatory standards.

2. What are pipeline artifacts and how are they used in DevOps workflows?
Artifacts are the files or outputs generated by a pipeline
Usage:
Stored after a build or processing stage so they can be used later (e.g., deploying, testing, or sharing).
Example: in your exercise, the pipeline saves raw_sales_data.csv and clean_sales_data.csv as artifacts.
Artifacts ensure traceability (you can always go back to the exact data/code that was used).
They allow multi-stage pipelines → one stage builds, another stage tests, another deploys, all using the same artifact.

3. How would you modify the pipeline to store the cleaned data into Azure Blob Storage?
- task: AzureCLI@2
  inputs:
    azureSubscription: '<Your-Service-Connection>'   # Service connection to Azure
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az storage blob upload \
        --account-name <your_storage_account> \
        --container-name <your_container> \
        --name clean_sales_data.csv \
        --file data/clean_sales_data.csv
