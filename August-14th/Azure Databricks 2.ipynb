{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bedb0d8d-8c6b-4c9c-b9f8-8782ec9e25a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"DeltaRideHailing\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    ")\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sql(\"USE CATALOG hive_metastore\")\n",
    "spark.sql(\"USE default\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c98ad9-23fb-4d09-884f-9cf4d0993044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeded:\n",
      " Trips   -> dbfs:/tmp/delta/ride_hailing/trips\n",
      " Drivers -> dbfs:/tmp/delta/ride_hailing/drivers\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "trip_schema = T.StructType([\n",
    "    T.StructField(\"trip_id\", T.IntegerType()),\n",
    "    T.StructField(\"rider_id\", T.StringType()),\n",
    "    T.StructField(\"driver_id\", T.StringType()),\n",
    "    T.StructField(\"city\", T.StringType()),\n",
    "    T.StructField(\"distance_km\", T.DoubleType()),\n",
    "    T.StructField(\"fare\", T.DoubleType()),\n",
    "    T.StructField(\"tip\", T.DoubleType()),\n",
    "    T.StructField(\"ts\", T.TimestampType())\n",
    "])\n",
    "\n",
    "driver_schema = T.StructType([\n",
    "    T.StructField(\"driver_id\", T.StringType()),\n",
    "    T.StructField(\"driver_name\", T.StringType()),\n",
    "    T.StructField(\"rating\", T.DoubleType()),\n",
    "    T.StructField(\"vehicle\", T.StringType())\n",
    "])\n",
    "\n",
    "trips_rows = [\n",
    "    (1001,\"R001\",\"D010\",\"Bengaluru\", 12.4, 320.0, 20.0, dt.datetime(2025,8,8,8,5)),\n",
    "    (1002,\"R002\",\"D011\",\"Hyderabad\", 6.2, 150.0, 10.0, dt.datetime(2025,8,8,8,15)),\n",
    "    (1003,\"R003\",\"D012\",\"Pune\", 3.5, 90.0, 0.0, dt.datetime(2025,8,8,8,20)),\n",
    "    (1004,\"R001\",\"D010\",\"Bengaluru\", 18.9, 480.0, 25.0, dt.datetime(2025,8,8,8,45)),\n",
    "    (1005,\"R004\",\"D013\",\"Chennai\", 10.0, 260.0, 15.0, dt.datetime(2025,8,8,9,5)),\n",
    "    (1006,\"R005\",\"D012\",\"Pune\", 2.2, 70.0, 0.0, dt.datetime(2025,8,8,9,10)),\n",
    "]\n",
    "\n",
    "drivers_rows = [\n",
    "    (\"D010\",\"Anil\",   4.8,\"WagonR\"),\n",
    "    (\"D011\",\"Sana\",   4.6,\"i20\"),\n",
    "    (\"D012\",\"Rakesh\", 4.4,\"Swift\"),\n",
    "    (\"D013\",\"Meera\",  4.9,\"Ciaz\")\n",
    "]\n",
    "\n",
    "trips_df   = spark.createDataFrame(trips_rows,   trip_schema)\n",
    "drivers_df = spark.createDataFrame(drivers_rows, driver_schema)\n",
    "\n",
    "BASE         = \"dbfs:/tmp/delta/ride_hailing\"\n",
    "TRIPS_PATH   = f\"{BASE}/trips\"\n",
    "DRIVERS_PATH = f\"{BASE}/drivers\"\n",
    "\n",
    "trips_df.write.format(\"delta\").mode(\"overwrite\").save(TRIPS_PATH)\n",
    "drivers_df.write.format(\"delta\").mode(\"overwrite\").save(DRIVERS_PATH)\n",
    "\n",
    "print(\"Seeded:\")\n",
    "print(\" Trips   ->\", TRIPS_PATH)\n",
    "print(\" Drivers ->\", DRIVERS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d9ce16-f6b1-4ed7-82cf-69f570d69fd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAIL trips_managed:\n",
      "+------+------------------------------------+------------------------------------+-----------+---------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "|format|id                                  |name                                |description|location                               |createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                           |minReaderVersion|minWriterVersion|tableFeatures                            |statistics                                                     |clusterByAuto|\n",
      "+------+------------------------------------+------------------------------------+-----------+---------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "|delta |543ce708-347e-4d77-a070-0b38c18b66b0|hive_metastore.default.trips_managed|NULL       |dbfs:/user/hive/warehouse/trips_managed|2025-08-14 06:18:22.632|2025-08-14 06:18:23|[]              |[]               |1       |2354       |{delta.enableDeletionVectors -> true}|3               |7               |[appendOnly, deletionVectors, invariants]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|false        |\n",
      "+------+------------------------------------+------------------------------------+-----------+---------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "\n",
      "DETAIL drivers_ext:\n",
      "+------+------------------------------------+----------------------------------+-----------+------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "|format|id                                  |name                              |description|location                            |createdAt              |lastModified       |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties                           |minReaderVersion|minWriterVersion|tableFeatures                            |statistics                                                     |clusterByAuto|\n",
      "+------+------------------------------------+----------------------------------+-----------+------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "|delta |82559c33-41ec-49d3-aa15-aa7c26728ccf|hive_metastore.default.drivers_ext|NULL       |dbfs:/tmp/delta/ride_hailing/drivers|2025-08-14 06:17:41.372|2025-08-14 06:17:41|[]              |[]               |1       |1321       |{delta.enableDeletionVectors -> true}|3               |7               |[appendOnly, deletionVectors, invariants]|{numRowsDeletedByDeletionVectors -> 0, numDeletionVectors -> 0}|false        |\n",
      "+------+------------------------------------+----------------------------------+-----------+------------------------------------+-----------------------+-------------------+----------------+-----------------+--------+-----------+-------------------------------------+----------------+----------------+-----------------------------------------+---------------------------------------------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Managed\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE trips_managed\n",
    "USING DELTA\n",
    "AS SELECT * FROM delta.`{TRIPS_PATH}`\n",
    "\"\"\")\n",
    "\n",
    "# Unmanaged\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS drivers_ext\n",
    "USING DELTA\n",
    "LOCATION '{DRIVERS_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"DETAIL trips_managed:\")\n",
    "spark.sql(\"DESCRIBE DETAIL trips_managed\").show(truncate=False)\n",
    "print(\"DETAIL drivers_ext:\")\n",
    "spark.sql(\"DESCRIBE DETAIL drivers_ext\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managed Table:databricks controls data storage and metadata\n",
    "unmanaged Table:databricks controls only metadata\n",
    "\n",
    "Managed Table:dropping the table will removes both the metadata and data files\n",
    "Unmanaged Table:dropping the table will keeps the data files we can read them directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc1350c-9c54-4647-bf6e-82fdb1cff641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2.Read & Explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948e67c7-f6db-4351-bdf9-1664670c2e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- trip_id: integer (nullable = true)\n",
      " |-- rider_id: string (nullable = true)\n",
      " |-- driver_id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- distance_km: double (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- ts: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- driver_id: string (nullable = true)\n",
      " |-- driver_name: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- vehicle: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read both tables\n",
    "trips_df = spark.table(\"trips_managed\")\n",
    "drivers_df = spark.table(\"drivers_ext\")\n",
    "\n",
    "# Print schemas\n",
    "trips_df.printSchema()\n",
    "drivers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13052d4e-4e29-4a30-87d4-7eccc638b253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|trip_id|rider_id|driver_id|city     |distance_km|fare |tip |ts                 |\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|1001   |R001    |D010     |Bengaluru|12.4       |320.0|20.0|2025-08-08 08:05:00|\n",
      "|1002   |R002    |D011     |Hyderabad|6.2        |150.0|10.0|2025-08-08 08:15:00|\n",
      "|1003   |R003    |D012     |Pune     |3.5        |90.0 |0.0 |2025-08-08 08:20:00|\n",
      "|1004   |R001    |D010     |Bengaluru|18.9       |480.0|25.0|2025-08-08 08:45:00|\n",
      "|1005   |R004    |D013     |Chennai  |10.0       |260.0|15.0|2025-08-08 09:05:00|\n",
      "|1006   |R005    |D012     |Pune     |2.2        |70.0 |0.0 |2025-08-08 09:10:00|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "\n",
      "+---------+-----------+------+-------+\n",
      "|driver_id|driver_name|rating|vehicle|\n",
      "+---------+-----------+------+-------+\n",
      "|D010     |Anil       |4.8   |WagonR |\n",
      "|D011     |Sana       |4.6   |i20    |\n",
      "|D012     |Rakesh     |4.4   |Swift  |\n",
      "|D013     |Meera      |4.9   |Ciaz   |\n",
      "+---------+-----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 10 rows\n",
    "trips_df.show(10, truncate=False)\n",
    "drivers_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac63c07-abb3-4472-bd6d-810c68a29c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+------------+\n",
      "|trip_id|rider_id|driver_id|city     |distance_km|fare |tip |ts                 |total_amount|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+------------+\n",
      "|1004   |R001    |D010     |Bengaluru|18.9       |480.0|25.0|2025-08-08 08:45:00|505.0       |\n",
      "|1001   |R001    |D010     |Bengaluru|12.4       |320.0|20.0|2025-08-08 08:05:00|340.0       |\n",
      "|1005   |R004    |D013     |Chennai  |10.0       |260.0|15.0|2025-08-08 09:05:00|275.0       |\n",
      "|1002   |R002    |D011     |Hyderabad|6.2        |150.0|10.0|2025-08-08 08:15:00|160.0       |\n",
      "|1003   |R003    |D012     |Pune     |3.5        |90.0 |0.0 |2025-08-08 08:20:00|90.0        |\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Add derived column total_amount = fare + tip\n",
    "trips_df = trips_df.withColumn(\"total_amount\", F.col(\"fare\") + F.col(\"tip\"))\n",
    "\n",
    "# Show top 5 trips by total_amount\n",
    "trips_df.orderBy(F.desc(\"total_amount\")).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2b340e7-ad34-4730-a195-d8aa628d7783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3) Update (Business Rule)\n",
    "Increase tip by 5 for trips in Bengaluru where distance_km > 15 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d6a144-578e-4f0e-9384-c44dbeaeeb77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-----+----+\n",
      "|trip_id|     city|distance_km| fare| tip|\n",
      "+-------+---------+-----------+-----+----+\n",
      "|   1004|Bengaluru|       18.9|480.0|25.0|\n",
      "+-------+---------+-----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show BEFORE\n",
    "spark.sql(\"\"\"\n",
    "SELECT trip_id, city, distance_km, fare, tip\n",
    "FROM trips_managed\n",
    "WHERE city = 'Bengaluru' AND distance_km > 15\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fc6ff2-1147-4abe-8751-50c9643999c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UPDATE\n",
    "spark.sql(\"\"\"\n",
    "UPDATE trips_managed\n",
    "SET tip = tip + 5\n",
    "WHERE city = 'Bengaluru' AND distance_km > 15\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9903676-9a3d-4466-8724-179c1ba7fad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+-----+----+\n",
      "|trip_id|     city|distance_km| fare| tip|\n",
      "+-------+---------+-----------+-----+----+\n",
      "|   1004|Bengaluru|       18.9|480.0|30.0|\n",
      "+-------+---------+-----------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show AFTER\n",
    "spark.sql(\"\"\"\n",
    "SELECT trip_id, city, distance_km, fare, tip\n",
    "FROM trips_managed\n",
    "WHERE city = 'Bengaluru' AND distance_km > 15\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db49bedb-44e9-49cf-b2b6-4555cd36ee1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4) Delete (Data Quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a4fa19-08b8-4d87-a040-945127925804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|remaining_rows|\n",
      "+--------------+\n",
      "|             6|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Delete Delete trips where fare <= 0 or distance_km <= 0 (simulate bad ingest).\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM trips_managed\n",
    "WHERE fare <= 0 OR distance_km <= 0\n",
    "\"\"\")\n",
    "\n",
    "# Show remaining row count\n",
    "spark.sql(\"\"\"\n",
    "SELECT COUNT(*) AS remaining_rows\n",
    "FROM trips_managed\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a960b201-6a49-4462-9a12-d8b7d5add60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5) Merge (Upsert New Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ee4466-33c0-49c4-b96a-50d6fc1ec94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|trip_id|rider_id|driver_id|     city|distance_km| fare| tip|                 ts|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|   1004|    R001|     D010|Bengaluru|       18.9|480.0|40.0|2025-08-08 08:45:00|\n",
      "|   1007|    R006|     D013|Hyderabad|        8.0|200.0|15.0|2025-08-08 10:00:00|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "merge_rows = [\n",
    "    Row(trip_id=1004, rider_id=\"R001\", driver_id=\"D010\", city=\"Bengaluru\",\n",
    "        distance_km=18.9, fare=480.0, tip=40.0, ts=\"2025-08-08 08:45:00\"),  # updated tip\n",
    "    Row(trip_id=1007, rider_id=\"R006\", driver_id=\"D013\", city=\"Hyderabad\",\n",
    "        distance_km=8.0, fare=200.0, tip=15.0, ts=\"2025-08-08 10:00:00\")    # new trip\n",
    "]\n",
    "\n",
    "merge_df = spark.createDataFrame(merge_rows)\n",
    "\n",
    "# Save to a temp Delta path\n",
    "MERGE_PATH = \"/tmp/delta/ride_hailing/merge_batch\"\n",
    "merge_df.write.format(\"delta\").mode(\"overwrite\").save(MERGE_PATH)\n",
    "\n",
    "# Merge into trips_managed\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO trips_managed AS t\n",
    "USING delta.`{MERGE_PATH}` AS s\n",
    "ON t.trip_id = s.trip_id\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Confirm the update & insert\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM trips_managed\n",
    "WHERE trip_id IN (1004, 1007)\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d014376e-d227-434a-9680-a207f9260b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6) Gold View (Join & KPIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fefff59c-8fb1-407b-8846-15431c82e4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create gold view with join\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW gold_trips AS\n",
    "SELECT t.trip_id, t.city, d.driver_name, d.rating,\n",
    "       t.distance_km, (t.fare + t.tip) AS total_amount, t.ts\n",
    "FROM trips_managed t\n",
    "JOIN drivers_ext d\n",
    "  ON t.driver_id = d.driver_id\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de503d7-692c-4ed1-a91e-5c44e69a6a5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------------+\n",
      "|     city|total_revenue|avg_driver_rating|\n",
      "+---------+-------------+-----------------+\n",
      "|  Chennai|        275.0|              4.9|\n",
      "|     Pune|        160.0|              4.4|\n",
      "|Bengaluru|        860.0|              4.8|\n",
      "|Hyderabad|        375.0|             4.75|\n",
      "+---------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# City-wise total revenue & avg driver rating\n",
    "spark.sql(\"\"\"\n",
    "SELECT city,\n",
    "       ROUND(SUM(total_amount), 2) AS total_revenue,\n",
    "       ROUND(AVG(rating), 2) AS avg_driver_rating\n",
    "FROM gold_trips\n",
    "GROUP BY city\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1680b618-4e6f-483b-b901-7c78857c97c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------------+\n",
      "|driver_name|total_trips|total_revenue|\n",
      "+-----------+-----------+-------------+\n",
      "|       Anil|          2|        860.0|\n",
      "|      Meera|          2|        490.0|\n",
      "|     Rakesh|          2|        160.0|\n",
      "+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Driver-wise total trips & top 3 drivers by revenue\n",
    "spark.sql(\"\"\"\n",
    "SELECT driver_name,\n",
    "       COUNT(*) AS total_trips,\n",
    "       ROUND(SUM(total_amount), 2) AS total_revenue\n",
    "FROM gold_trips\n",
    "GROUP BY driver_name\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96d24ab4-1983-4efc-85a8-e6c62e7c8766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7) Time Travel & History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a4411be-0d70-4d4c-b6bd-635f59319201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|version|timestamp          |userId         |userName                          |operation                        |operationParameters                                                                                                                                                                                                                              |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |userMetadata|engineInfo                                |\n",
      "+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "|5      |2025-08-14 06:25:57|149052920012809|azuser4012_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                                                   |NULL|{1364532786276870}|0806-113651-tuywd28y|4          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4458, p25FileSize -> 2382, numDeletionVectorsRemoved -> 1, minFileSize -> 2382, numAddedFiles -> 1, maxFileSize -> 2382, p75FileSize -> 2382, p50FileSize -> 2382, numAddedBytes -> 2382}                                                                                                                                                                                                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "|4      |2025-08-14 06:25:54|149052920012809|azuser4012_mml.local@techademy.com|MERGE                            |{predicate -> [\"(cast(trip_id#14583 as bigint) = trip_id#14599L)\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}]}|NULL|{1364532786276870}|0806-113651-tuywd28y|3          |WriteSerializable|false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2101, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 1, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 2044, materializeSourceTimeMs -> 2, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 848, numTargetRowsUpdated -> 1, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1158}|NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "|3      |2025-08-14 06:24:56|149052920012809|azuser4012_mml.local@techademy.com|DELETE                           |{predicate -> [\"((fare#14040 <= 0.0) OR (distance_km#14039 <= 0.0))\"]}                                                                                                                                                                           |NULL|{1364532786276870}|0806-113651-tuywd28y|2          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 221, numDeletionVectorsUpdated -> 0, numDeletedRows -> 0, scanTimeMs -> 221, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0}                                                                                                                                                                                                                                                                                                                                                              |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "|2      |2025-08-14 06:22:40|149052920012809|azuser4012_mml.local@techademy.com|OPTIMIZE                         |{predicate -> [], auto -> true, clusterBy -> [], zOrderBy -> [], batchId -> 0}                                                                                                                                                                   |NULL|{1364532786276870}|0806-113651-tuywd28y|1          |SnapshotIsolation|false        |{numRemovedFiles -> 2, numRemovedBytes -> 4379, p25FileSize -> 2357, numDeletionVectorsRemoved -> 1, minFileSize -> 2357, numAddedFiles -> 1, maxFileSize -> 2357, p75FileSize -> 2357, p50FileSize -> 2357, numAddedBytes -> 2357}                                                                                                                                                                                                                                                                                                                                                                                                                                               |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "|1      |2025-08-14 06:22:38|149052920012809|azuser4012_mml.local@techademy.com|UPDATE                           |{predicate -> [\"((city#13057 = Bengaluru) AND (distance_km#13058 > 15.0))\"]}                                                                                                                                                                     |NULL|{1364532786276870}|0806-113651-tuywd28y|0          |WriteSerializable|false        |{numRemovedFiles -> 0, numRemovedBytes -> 0, numCopiedRows -> 0, numDeletionVectorsAdded -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 1382, numDeletionVectorsUpdated -> 0, scanTimeMs -> 603, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 2025, rewriteTimeMs -> 778}                                                                                                                                                                                                                                                                                                                                                        |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "|0      |2025-08-14 06:18:23|149052920012809|azuser4012_mml.local@techademy.com|CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false}                                                                                         |NULL|{1364532786276870}|0806-113651-tuywd28y|NULL       |WriteSerializable|false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 6, numOutputBytes -> 2354}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |NULL        |Databricks-Runtime/16.4.x-photon-scala2.12|\n",
      "+-------+-------------------+---------------+----------------------------------+---------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+------------------------------------------+\n",
      "\n",
      "Version 0 count: 6\n",
      "Latest count: 7\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|trip_id|rider_id|driver_id|     city|distance_km| fare| tip|                 ts|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|   1001|    R001|     D010|Bengaluru|       12.4|320.0|20.0|2025-08-08 08:05:00|\n",
      "|   1002|    R002|     D011|Hyderabad|        6.2|150.0|10.0|2025-08-08 08:15:00|\n",
      "|   1003|    R003|     D012|     Pune|        3.5| 90.0| 0.0|2025-08-08 08:20:00|\n",
      "|   1004|    R001|     D010|Bengaluru|       18.9|480.0|25.0|2025-08-08 08:45:00|\n",
      "|   1005|    R004|     D013|  Chennai|       10.0|260.0|15.0|2025-08-08 09:05:00|\n",
      "|   1006|    R005|     D012|     Pune|        2.2| 70.0| 0.0|2025-08-08 09:10:00|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|trip_id|rider_id|driver_id|     city|distance_km| fare| tip|                 ts|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|   1001|    R001|     D010|Bengaluru|       12.4|320.0|20.0|2025-08-08 08:05:00|\n",
      "|   1002|    R002|     D011|Hyderabad|        6.2|150.0|10.0|2025-08-08 08:15:00|\n",
      "|   1003|    R003|     D012|     Pune|        3.5| 90.0| 0.0|2025-08-08 08:20:00|\n",
      "|   1005|    R004|     D013|  Chennai|       10.0|260.0|15.0|2025-08-08 09:05:00|\n",
      "|   1006|    R005|     D012|     Pune|        2.2| 70.0| 0.0|2025-08-08 09:10:00|\n",
      "|   1004|    R001|     D010|Bengaluru|       18.9|480.0|40.0|2025-08-08 08:45:00|\n",
      "|   1007|    R006|     D013|Hyderabad|        8.0|200.0|15.0|2025-08-08 10:00:00|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show table history\n",
    "spark.sql(\"DESCRIBE HISTORY trips_managed\").show(truncate=False)\n",
    "\n",
    "# Read as of version 0\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"trips_managed\")\n",
    "print(\"Version 0 count:\", df_v0.count())\n",
    "\n",
    "# Compare with latest\n",
    "df_latest = spark.table(\"trips_managed\")\n",
    "print(\"Latest count:\", df_latest.count())\n",
    "\n",
    "df_v0.show()\n",
    "df_latest.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10883a6d-d1e2-4176-9b1a-125bf2a1e109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8) Partitioned Rewrite (Performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd857018-32f0-439c-b4ef-d603fff5a397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|trip_id|rider_id|driver_id|     city|distance_km| fare| tip|                 ts|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "|   1004|    R001|     D010|Bengaluru|       18.9|480.0|40.0|2025-08-08 08:45:00|\n",
      "|   1001|    R001|     D010|Bengaluru|       12.4|320.0|20.0|2025-08-08 08:05:00|\n",
      "+-------+--------+---------+---------+-----------+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New path for partitioned table\n",
    "PARTITIONED_PATH = \"/tmp/delta/ride_hailing/trips_partitioned\"\n",
    "\n",
    "# Rewrite with partition by city\n",
    "spark.table(\"trips_managed\") \\\n",
    "     .write.format(\"delta\") \\\n",
    "     .mode(\"overwrite\") \\\n",
    "     .partitionBy(\"city\") \\\n",
    "     .save(PARTITIONED_PATH)\n",
    "\n",
    "# Simple filter to observe read behavior\n",
    "spark.read.format(\"delta\").load(PARTITIONED_PATH) \\\n",
    "     .filter(\"city = 'Bengaluru'\") \\\n",
    "     .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd955f0-632d-4670-8a66-0dd2ab9bf1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9) Incremental Load Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883ddaee-df2d-47c5-8dce-aa72f2520ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "inc_batch = [\n",
    "    (1008, \"R007\", \"D013\", \"Mumbai\", 7.0, 180.0, 10.0, datetime.strptime(\"2025-08-14 11:00:00\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (1009, \"R008\", \"D011\", \"Mumbai\", 5.0, 120.0, 5.0, datetime.strptime(\"2025-08-14 11:15:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "inc_df = spark.createDataFrame(inc_batch, schema=trip_schema)\n",
    "\n",
    "# Append to Delta table\n",
    "inc_df.write.format(\"delta\").mode(\"append\").save(TRIPS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b1a38ae-c035-4134-9915-e0e99eb4ff01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|     city|sum(total_amount)|\n",
      "+---------+-----------------+\n",
      "|  Chennai|            275.0|\n",
      "|     Pune|            160.0|\n",
      "|Bengaluru|            845.0|\n",
      "|Hyderabad|            160.0|\n",
      "|   Mumbai|            315.0|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-run city revenue\n",
    "spark.read.format(\"delta\").load(TRIPS_PATH).withColumn(\"total_amount\", F.col(\"fare\")+F.col(\"tip\")) \\\n",
    "    .groupBy(\"city\").sum(\"total_amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8072824e-412c-45b0-905c-6b71a2e0ef06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10) Simple Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb8e423-fab3-459f-afdd-32e79a5a36a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_df = spark.readStream.format(\"delta\").load(TRIPS_PATH)\n",
    "\n",
    "query = stream_df.writeStream.format(\"console\").outputMode(\"append\").trigger(processingTime='5 seconds').start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98005c10-78b6-4326-bba4-0d4fa6d9dcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d82c7d4-f014-4258-9544-18402d813758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11) Visualization (Pandas + Matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a326a909-caf9-4601-9cd9-ed0009648409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read Delta tables\n",
    "trips = spark.read.format(\"delta\").load(TRIPS_PATH)\n",
    "drivers = spark.read.format(\"delta\").load(DRIVERS_PATH)\n",
    "\n",
    "# Create gold DataFrame\n",
    "gold = trips.alias(\"t\").join(drivers.alias(\"d\"), \"driver_id\") \\\n",
    "    .withColumn(\"total_amount\", F.col(\"fare\") + F.col(\"tip\")) \\\n",
    "    .select(\"trip_id\", \"city\", \"driver_name\", \"rating\", \"distance_km\", \"total_amount\", \"ts\")\n",
    "\n",
    "# Convert to Pandas for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "city_rev = gold.toPandas().groupby(\"city\")[\"total_amount\"].sum().reset_index()\n",
    "\n",
    "plt.bar(city_rev['city'], city_rev['total_amount'])\n",
    "plt.title(\"City Revenue\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9bd6399-0640-40cd-ae31-8e1f0ef9b9c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12) Managed vs Unmanaged Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d7e5f2d-f905-411a-9afd-1ab603ad6814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE trips_managed\")\n",
    "spark.sql(\"DROP TABLE drivers_ext\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ba69f9-0b6c-41c7-a417-30764cb0038e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13) Constraint/Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e616ece-0bca-4d32-8adb-0c1a44d6c131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a managed Delta table for quality check\n",
    "CREATE TABLE trips_managed_qc\n",
    "USING DELTA\n",
    "LOCATION '/tmp/delta/ride_hailing/trips_qc'\n",
    "AS\n",
    "SELECT * FROM delta.`/tmp/delta/ride_hailing/trips`;\n",
    "\n",
    "-- Add a CHECK constraint \n",
    "ALTER TABLE trips_managed_qc\n",
    "ADD CONSTRAINT tip_nonnegative CHECK (tip >= 0);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e490bf40-17d9-4575-a21c-3e2231972874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-6566768326298991>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- This insert will fail due to the CHECK constraint\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mINSERT INTO trips_managed_qc VALUES\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(1010, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR009\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD014\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBengaluru\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, 10.0, 200.0, -5.0, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-08-14 12:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n",
       "\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n",
       "\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n",
       "\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n",
       "\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n",
       "\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n",
       "\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:130\u001b[0m, in \u001b[0;36mSqlMagic.sql\u001b[0;34m(self, line, cell)\u001b[0m\n",
       "\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n",
       "\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m    128\u001b[0m     )\n",
       "\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_py4j:\n",
       "\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_via_py4j(cell)\n",
       "\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_via_sql_comm_handler(cell)\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:217\u001b[0m, in \u001b[0;36mSqlMagic.execute_via_py4j\u001b[0;34m(self, cell)\u001b[0m\n",
       "\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\n",
       "\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    214\u001b[0m         exceptionClassName\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n",
       "\u001b[1;32m    215\u001b[0m         sqlState\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetSqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
       "\u001b[1;32m    216\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetErrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
       "\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:210\u001b[0m, in \u001b[0;36mSqlMagic.execute_via_py4j\u001b[0;34m(self, cell)\u001b[0m\n",
       "\u001b[1;32m    208\u001b[0m         query_text \u001b[38;5;241m=\u001b[39m sub_query\u001b[38;5;241m.\u001b[39mquery()\n",
       "\u001b[1;32m    209\u001b[0m         sql_directive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point\u001b[38;5;241m.\u001b[39mgetSqlDirective(query_text)\n",
       "\u001b[0;32m--> 210\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_sql_directive(sql_directive, i \u001b[38;5;241m==\u001b[39m number_of_sub_queries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
       "\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\n",
       "\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m    214\u001b[0m         exceptionClassName\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n",
       "\u001b[1;32m    215\u001b[0m         sqlState\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetSqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n",
       "\u001b[1;32m    216\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetErrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:235\u001b[0m, in \u001b[0;36mSqlMagic.handle_sql_directive\u001b[0;34m(self, sql_directive, is_last_query)\u001b[0m\n",
       "\u001b[1;32m    233\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_request_result(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m directive_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoDirective\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
       "\u001b[0;32m--> 235\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_request_result(sql_directive\u001b[38;5;241m.\u001b[39msql())\n",
       "\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001b[39;00m\n",
       "\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:107\u001b[0m, in \u001b[0;36mSqlMagic.get_query_request_result\u001b[0;34m(self, query)\u001b[0m\n",
       "\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(widget_bindings \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mgetAll()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
       "\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARAM_SYNTAX_USAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[0;32m--> 107\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql(query, widget_bindings)\n",
       "\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:821\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    818\u001b[0m         _views\u001b[38;5;241m.\u001b[39mappend(SubqueryAlias(df\u001b[38;5;241m.\u001b[39m_plan, name))\n",
       "\u001b[1;32m    820\u001b[0m cmd \u001b[38;5;241m=\u001b[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001b[0;32m--> 821\u001b[0m data, properties, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client))\n",
       "\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n",
       "\u001b[1;32m    823\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(CachedRelation(properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1481\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n",
       "\u001b[1;32m   1479\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n",
       "\u001b[1;32m   1480\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n",
       "\u001b[0;32m-> 1481\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n",
       "\u001b[1;32m   1482\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n",
       "\u001b[1;32m   1483\u001b[0m )\n",
       "\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n",
       "\u001b[1;32m   1485\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1970\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n",
       "\u001b[1;32m   1967\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n",
       "\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n",
       "\u001b[0;32m-> 1970\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n",
       "\u001b[1;32m   1971\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n",
       "\u001b[1;32m   1972\u001b[0m     ):\n",
       "\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n",
       "\u001b[1;32m   1974\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1946\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n",
       "\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n",
       "\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
       "\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2266\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n",
       "\u001b[1;32m   2264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
       "\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n",
       "\u001b[0;32m-> 2266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n",
       "\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
       "\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
       "\n",
       "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2377\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n",
       "\u001b[1;32m   2363\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n",
       "\u001b[1;32m   2364\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m   2365\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m   2373\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39mSparkConnectGrpcException\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n",
       "\u001b[1;32m   2374\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2375\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n",
       "\u001b[0;32m-> 2377\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n",
       "\u001b[1;32m   2378\u001b[0m                 info,\n",
       "\u001b[1;32m   2379\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n",
       "\u001b[1;32m   2380\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n",
       "\u001b[1;32m   2381\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n",
       "\u001b[1;32m   2382\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n",
       "\u001b[1;32m   2385\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n",
       "\u001b[1;32m   2386\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mSparkConnectGrpcException\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n",
       "\u001b[1;32m   2387\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\n",
       "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException) [DELTA_VIOLATE_CONSTRAINT_WITH_VALUES] CHECK constraint tip_nonnegative (tip >= 0) violated by row with values:\n",
       " - tip : -5.0\n",
       "\n",
       "JVM stacktrace:\n",
       "com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException\n",
       "\tat com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException$.getConstraintViolationWithValuesException(InvariantViolationException.scala:82)\n",
       "\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:333)\n",
       "\tat com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)\n",
       "\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)\n",
       "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan(TransactionalWriteEdge.scala:903)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan$(TransactionalWriteEdge.scala:883)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetExecutedPlan(OptimisticTransaction.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.writeFilesWithoutClustering(ClusteredWriter.scala:1012)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runInternal(ClusteredWriter.scala:259)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runAndReturnMaterializationPlans(ClusteredWriter.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeFilesAndGetMaterializationPlans(WriteIntoDeltaEdge.scala:646)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:579)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:209)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$2(WriteIntoDeltaEdge.scala:155)\n",
       "\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$1(WriteIntoDeltaEdge.scala:142)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.run(WriteIntoDeltaEdge.scala:141)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$1$$anon$2.insert(DeltaTableV2.scala:795)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:137)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:118)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:36)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:84)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:82)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:36)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkConnectGrpcException",
        "evalue": "(com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException) [DELTA_VIOLATE_CONSTRAINT_WITH_VALUES] CHECK constraint tip_nonnegative (tip >= 0) violated by row with values:\n - tip : -5.0\n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException\n\tat com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException$.getConstraintViolationWithValuesException(InvariantViolationException.scala:82)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:333)\n\tat com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan(TransactionalWriteEdge.scala:903)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan$(TransactionalWriteEdge.scala:883)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetExecutedPlan(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.writeFilesWithoutClustering(ClusteredWriter.scala:1012)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runInternal(ClusteredWriter.scala:259)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runAndReturnMaterializationPlans(ClusteredWriter.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeFilesAndGetMaterializationPlans(WriteIntoDeltaEdge.scala:646)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:579)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$2(WriteIntoDeltaEdge.scala:155)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$1(WriteIntoDeltaEdge.scala:142)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.run(WriteIntoDeltaEdge.scala:141)\n\tat com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$1$$anon$2.insert(DeltaTableV2.scala:795)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:137)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:118)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:82)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       },
       "metadata": {
        "errorSummary": "[DELTA_VIOLATE_CONSTRAINT_WITH_VALUES] CHECK constraint tip_nonnegative (tip >= 0) violated by row with values:\n - tip : -5.0 SQLSTATE: 23001"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "DELTA_VIOLATE_CONSTRAINT_WITH_VALUES",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "23001",
        "stackTrace": "com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException\n\tat com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException$.getConstraintViolationWithValuesException(InvariantViolationException.scala:82)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:333)\n\tat com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan(TransactionalWriteEdge.scala:903)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan$(TransactionalWriteEdge.scala:883)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetExecutedPlan(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.writeFilesWithoutClustering(ClusteredWriter.scala:1012)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runInternal(ClusteredWriter.scala:259)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runAndReturnMaterializationPlans(ClusteredWriter.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeFilesAndGetMaterializationPlans(WriteIntoDeltaEdge.scala:646)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:579)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$2(WriteIntoDeltaEdge.scala:155)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$1(WriteIntoDeltaEdge.scala:142)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.run(WriteIntoDeltaEdge.scala:141)\n\tat com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$1$$anon$2.insert(DeltaTableV2.scala:795)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:137)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:118)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:82)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mSparkConnectGrpcException\u001b[0m                 Traceback (most recent call last)",
        "File \u001b[0;32m<command-6566768326298991>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_cell_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-- This insert will fail due to the CHECK constraint\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mINSERT INTO trips_managed_qc VALUES\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(1010, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR009\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD014\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBengaluru\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, 10.0, 200.0, -5.0, TIMESTAMP \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-08-14 12:00:00\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m);\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:130\u001b[0m, in \u001b[0;36mSqlMagic.sql\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_py4j:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_via_py4j(cell)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_via_sql_comm_handler(cell)\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:217\u001b[0m, in \u001b[0;36mSqlMagic.execute_via_py4j\u001b[0;34m(self, cell)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    214\u001b[0m         exceptionClassName\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    215\u001b[0m         sqlState\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetSqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    216\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetErrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:210\u001b[0m, in \u001b[0;36mSqlMagic.execute_via_py4j\u001b[0;34m(self, cell)\u001b[0m\n\u001b[1;32m    208\u001b[0m         query_text \u001b[38;5;241m=\u001b[39m sub_query\u001b[38;5;241m.\u001b[39mquery()\n\u001b[1;32m    209\u001b[0m         sql_directive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point\u001b[38;5;241m.\u001b[39mgetSqlDirective(query_text)\n\u001b[0;32m--> 210\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_sql_directive(sql_directive, i \u001b[38;5;241m==\u001b[39m number_of_sub_queries \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL_MAGIC_PY4J_FAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    214\u001b[0m         exceptionClassName\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    215\u001b[0m         sqlState\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetSqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    216\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39msafe_call(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetErrorClass\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:235\u001b[0m, in \u001b[0;36mSqlMagic.handle_sql_directive\u001b[0;34m(self, sql_directive, is_last_query)\u001b[0m\n\u001b[1;32m    233\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_request_result(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m directive_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNoDirective\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 235\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_query_request_result(sql_directive\u001b[38;5;241m.\u001b[39msql())\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:107\u001b[0m, in \u001b[0;36mSqlMagic.get_query_request_result\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(widget_bindings \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbutils\u001b[38;5;241m.\u001b[39mwidgets\u001b[38;5;241m.\u001b[39mgetAll()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_activity_logger\u001b[38;5;241m.\u001b[39mlogExecuteCommandEvent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARAM_SYNTAX_USAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 107\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql(query, widget_bindings)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:821\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m         _views\u001b[38;5;241m.\u001b[39mappend(SubqueryAlias(df\u001b[38;5;241m.\u001b[39m_plan, name))\n\u001b[1;32m    820\u001b[0m cmd \u001b[38;5;241m=\u001b[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001b[0;32m--> 821\u001b[0m data, properties, ei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mexecute_command(cmd\u001b[38;5;241m.\u001b[39mcommand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client))\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m properties:\n\u001b[1;32m    823\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(CachedRelation(properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msql_command_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;28mself\u001b[39m)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1481\u001b[0m, in \u001b[0;36mSparkConnectClient.execute_command\u001b[0;34m(self, command, observations, extra_request_metadata)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     req\u001b[38;5;241m.\u001b[39muser_context\u001b[38;5;241m.\u001b[39muser_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_id\n\u001b[1;32m   1480\u001b[0m req\u001b[38;5;241m.\u001b[39mplan\u001b[38;5;241m.\u001b[39mcommand\u001b[38;5;241m.\u001b[39mCopyFrom(command)\n\u001b[0;32m-> 1481\u001b[0m data, _, metrics, observed_metrics, properties \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch(\n\u001b[1;32m   1482\u001b[0m     req, observations \u001b[38;5;129;01mor\u001b[39;00m {}, extra_request_metadata\n\u001b[1;32m   1483\u001b[0m )\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m ei \u001b[38;5;241m=\u001b[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1970\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch\u001b[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[0m\n\u001b[1;32m   1967\u001b[0m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_progress_handlers, operation_id\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39moperation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[0;32m-> 1970\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_and_fetch_as_iterator(\n\u001b[1;32m   1971\u001b[0m         req, observations, extra_request_metadata \u001b[38;5;129;01mor\u001b[39;00m [], progress\u001b[38;5;241m=\u001b[39mprogress\n\u001b[1;32m   1972\u001b[0m     ):\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, StructType):\n\u001b[1;32m   1974\u001b[0m             schema \u001b[38;5;241m=\u001b[39m response\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1946\u001b[0m, in \u001b[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[0;34m(self, req, observations, extra_request_metadata, progress)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2266\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc\u001b[38;5;241m.\u001b[39mRpcError):\n\u001b[0;32m-> 2266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_rpc_error(error)\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
        "File \u001b[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2377\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_rpc_error\u001b[0;34m(self, rpc_error)\u001b[0m\n\u001b[1;32m   2363\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[1;32m   2364\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPython versions in the Spark Connect client and server are different. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2365\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo execute user-defined functions, client and server should have the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2373\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlState\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39mSparkConnectGrpcException\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE),\n\u001b[1;32m   2374\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m             \u001b[38;5;66;03m# END-EDGE\u001b[39;00m\n\u001b[0;32m-> 2377\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[1;32m   2378\u001b[0m                 info,\n\u001b[1;32m   2379\u001b[0m                 status\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2380\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetch_enriched_error(info),\n\u001b[1;32m   2381\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_server_stack_trace(),\n\u001b[1;32m   2382\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[1;32m   2385\u001b[0m         message\u001b[38;5;241m=\u001b[39mstatus\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m   2386\u001b[0m         sql_state\u001b[38;5;241m=\u001b[39mSparkConnectGrpcException\u001b[38;5;241m.\u001b[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[1;32m   2387\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2388\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
        "\u001b[0;31mSparkConnectGrpcException\u001b[0m: (com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException) [DELTA_VIOLATE_CONSTRAINT_WITH_VALUES] CHECK constraint tip_nonnegative (tip >= 0) violated by row with values:\n - tip : -5.0\n\nJVM stacktrace:\ncom.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException\n\tat com.databricks.sql.transaction.tahoe.schema.DeltaInvariantViolationException$.getConstraintViolationWithValuesException(InvariantViolationException.scala:82)\n\tat com.databricks.photon.PhotonException$.getSparkException(PhotonException.scala:333)\n\tat com.databricks.photon.PhotonWriteResultHandler.getResult(PhotonWriteStageExec.scala:145)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.open(PhotonBasicEvaluatorFactory.scala:252)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNextImpl(PhotonBasicEvaluatorFactory.scala:257)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.$anonfun$hasNext$1(PhotonBasicEvaluatorFactory.scala:275)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat org.apache.spark.TaskContext.runFuncAsBillable(TaskContext.scala:268)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:275)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$3(FileFormatWriter.scala:674)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$2(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:76)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:227)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:204)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:166)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:160)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:105)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$11(Executor.scala:1224)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:112)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1228)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:1080)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1520)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1504)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3250)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:3231)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$2(FileFormatWriter.scala:671)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.ignoreMaxResultLimit(FileFormatWriter.scala:841)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeUsingPhoton$1(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.catalyst.MetricKeyUtils$.measureMs(MetricKey.scala:1701)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeUsingPhoton(FileFormatWriter.scala:669)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:292)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:405)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$1(FileFormatWriter.scala:282)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:126)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaCommand.run(WriteIntoDeltaCommand.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$5(commands.scala:137)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.$anonfun$sideEffectResult$4(commands.scala:137)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:133)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:132)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:149)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$17(TransactionalWriteEdge.scala:722)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFilesAndGetQueryExecution$1(TransactionalWriteEdge.scala:716)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:325)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:312)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:178)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:418)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:416)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:177)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:293)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:289)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:120)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:210)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:169)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:176)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:166)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:155)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:330)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:329)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution(TransactionalWriteEdge.scala:388)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetQueryExecution$(TransactionalWriteEdge.scala:378)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetQueryExecution(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan(TransactionalWriteEdge.scala:903)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFilesAndGetExecutedPlan$(TransactionalWriteEdge.scala:883)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFilesAndGetExecutedPlan(OptimisticTransaction.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.writeFilesWithoutClustering(ClusteredWriter.scala:1012)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runInternal(ClusteredWriter.scala:259)\n\tat com.databricks.sql.transaction.tahoe.commands.ClusteredWriter.runAndReturnMaterializationPlans(ClusteredWriter.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeFilesAndGetMaterializationPlans(WriteIntoDeltaEdge.scala:646)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:579)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:209)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$2(WriteIntoDeltaEdge.scala:155)\n\tat com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:302)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.$anonfun$run$1(WriteIntoDeltaEdge.scala:142)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2456)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:63)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:60)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:220)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2456)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.run(WriteIntoDeltaEdge.scala:141)\n\tat com.databricks.sql.transaction.tahoe.catalog.WriteIntoDeltaBuilder$$anon$1$$anon$2.insert(DeltaTableV2.scala:795)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1(V1FallbackWriters.scala:137)\n\tat org.apache.spark.sql.execution.datasources.v2.SupportsV1Write.writeWithV1$(V1FallbackWriters.scala:118)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.writeWithV1(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run(V1FallbackWriters.scala:84)\n\tat org.apache.spark.sql.execution.datasources.v2.V1FallbackWriters.run$(V1FallbackWriters.scala:82)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExecV1.run(V1FallbackWriters.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:194)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:450)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:450)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:449)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$11(SQLExecution.scala:478)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:414)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:777)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:344)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:213)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:714)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:445)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1307)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:370)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:439)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:503)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:521)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:497)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:498)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:498)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:326)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:331)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:406)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$3(Dataset.scala:150)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1469)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1469)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:141)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1094)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3606)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3435)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3370)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:413)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:312)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:464)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1462)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:464)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:90)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:89)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:463)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:233)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:139)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:614)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:614)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#bad data entry\n",
    "%sql\n",
    "-- This insert will fail due to the CHECK constraint\n",
    "INSERT INTO trips_managed_qc VALUES\n",
    "(1010, 'R009', 'D014', 'Bengaluru', 10.0, 200.0, -5.0, TIMESTAMP '2025-08-14 12:00:00');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202179a1-bb2f-4179-b08e-e140f6d59b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 40
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO trips_managed_qc VALUES\n",
    "(1011, 'R010', 'D015', 'Mumbai', 8.0, 180.0, 10.0, TIMESTAMP '2025-08-14 12:30:00');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c785293e-b7f2-47ce-a86a-7f2f686076b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14) Convert Parquet  Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35d5a10-03c5-4887-9774-4e843feafc18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>trip_id</th><th>rider_id</th><th>driver_id</th><th>city</th><th>distance_km</th><th>fare</th><th>tip</th><th>ts</th></tr></thead><tbody><tr><td>1001</td><td>R001</td><td>D010</td><td>Bengaluru</td><td>12.4</td><td>320.0</td><td>20.0</td><td>2025-08-08T08:05:00Z</td></tr><tr><td>1002</td><td>R002</td><td>D011</td><td>Hyderabad</td><td>6.2</td><td>150.0</td><td>10.0</td><td>2025-08-08T08:15:00Z</td></tr><tr><td>1003</td><td>R003</td><td>D012</td><td>Pune</td><td>3.5</td><td>90.0</td><td>0.0</td><td>2025-08-08T08:20:00Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1001,
         "R001",
         "D010",
         "Bengaluru",
         12.4,
         320,
         20,
         "2025-08-08T08:05:00Z"
        ],
        [
         1002,
         "R002",
         "D011",
         "Hyderabad",
         6.2,
         150,
         10,
         "2025-08-08T08:15:00Z"
        ],
        [
         1003,
         "R003",
         "D012",
         "Pune",
         3.5,
         90,
         0,
         "2025-08-08T08:20:00Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "trip_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "rider_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "driver_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "distance_km",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "fare",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "tip",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "ts",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save a small subset of trips as Parquet\n",
    "trips_df = spark.read.format(\"delta\").load(\"/tmp/delta/ride_hailing/trips\")\n",
    "parquet_path = \"/tmp/delta/ride_hailing/trips_parquet\"\n",
    "trips_df.limit(3).write.mode(\"overwrite\").format(\"delta\").save(parquet_path)\n",
    "\n",
    "# Convert Parquet to Delta\n",
    "spark.sql(f\"CONVERT TO DELTA parquet.`{parquet_path}`\")\n",
    "\n",
    "# Load as Delta\n",
    "delta_table = DeltaTable.forPath(spark, parquet_path)\n",
    "\n",
    "# MERGE example (upsert)\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "new_batch = [\n",
    "    (1012, \"R011\", \"D011\", \"Hyderabad\", 6.5, 150.0, 10.0, datetime.strptime(\"2025-08-14 12:45:00\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "]\n",
    "new_df = spark.createDataFrame(new_batch, schema=trip_schema)\n",
    "\n",
    "delta_table.alias(\"t\").merge(\n",
    "    new_df.alias(\"n\"),\n",
    "    \"t.trip_id = n.trip_id\"\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Time travel example: read version 0\n",
    "version0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(parquet_path)\n",
    "display(version0_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7436ad07-5f27-4f83-8c7f-17ae4023dd55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15) Bonus KPI Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27a34b8-b06e-432f-b021-ad725c471508",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>total_revenue</th></tr></thead><tbody><tr><td>1755.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1755
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load trips and drivers\n",
    "trips = spark.read.format(\"delta\").load(\"/tmp/delta/ride_hailing/trips\")\n",
    "drivers = spark.read.format(\"delta\").load(\"/tmp/delta/ride_hailing/drivers\")\n",
    "\n",
    "# Gold view\n",
    "gold = trips.alias(\"t\").join(drivers.alias(\"d\"), \"driver_id\") \\\n",
    "    .withColumn(\"total_amount\", F.col(\"fare\") + F.col(\"tip\")) \\\n",
    "    .select(\"trip_id\", \"city\", \"driver_name\", \"total_amount\", \"ts\")\n",
    "\n",
    "gold.createOrReplaceTempView(\"gold_view\")\n",
    "\n",
    "# Total revenue\n",
    "display(spark.sql(\"SELECT SUM(total_amount) AS total_revenue FROM gold_view\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e77176bb-4bf7-4a0c-88f0-0a2cf1d686c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>city</th><th>revenue</th></tr></thead><tbody><tr><td>Bengaluru</td><td>845.0</td></tr><tr><td>Mumbai</td><td>315.0</td></tr><tr><td>Chennai</td><td>275.0</td></tr><tr><td>Pune</td><td>160.0</td></tr><tr><td>Hyderabad</td><td>160.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Bengaluru",
         845
        ],
        [
         "Mumbai",
         315
        ],
        [
         "Chennai",
         275
        ],
        [
         "Pune",
         160
        ],
        [
         "Hyderabad",
         160
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trips per city \n",
    "city_rev_df = spark.sql(\"\"\"\n",
    "SELECT city, SUM(total_amount) AS revenue\n",
    "FROM gold_view\n",
    "GROUP BY city\n",
    "ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "display(city_rev_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67879136-6f75-41af-b6c5-3c585350551f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>driver_name</th><th>revenue</th></tr></thead><tbody><tr><td>Anil</td><td>845.0</td></tr><tr><td>Meera</td><td>465.0</td></tr><tr><td>Sana</td><td>285.0</td></tr><tr><td>Rakesh</td><td>160.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Anil",
         845
        ],
        [
         "Meera",
         465
        ],
        [
         "Sana",
         285
        ],
        [
         "Rakesh",
         160
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "driver_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top drivers by revenue\n",
    "driver_rev_df = spark.sql(\"\"\"\n",
    "SELECT driver_name, SUM(total_amount) AS revenue\n",
    "FROM gold_view\n",
    "GROUP BY driver_name\n",
    "ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "display(driver_rev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aea097e-018d-4cd0-9611-012cb2d39416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>hour</th><th>sum(total_amount)</th></tr></thead><tbody><tr><td>8</td><td>1095.0</td></tr><tr><td>9</td><td>345.0</td></tr><tr><td>11</td><td>315.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         8,
         1095
        ],
        [
         9,
         345
        ],
        [
         11,
         315
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "hour",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "sum(total_amount)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Revenue by hour\n",
    "hourly_rev_df = gold.withColumn(\"hour\", F.hour(\"ts\")) \\\n",
    "                    .groupBy(\"hour\") \\\n",
    "                    .sum(\"total_amount\") \\\n",
    "                    .orderBy(\"hour\")\n",
    "display(hourly_rev_df) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6566768326298992,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-08-14 11:40:51",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
