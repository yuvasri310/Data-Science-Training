{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. PySpark Setup & Initialization"
      ],
      "metadata": {
        "id": "PRX8sJGElAb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOv3x3cWk0kF",
        "outputId": "8afe1856-4186-4d75-ddfd-ddcfe06becb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BotCampus Intermediate Session\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load starter data\n",
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "        (\"Ravi\", \"Hyderabad\", 28),\n",
        "        (\"Kavya\", \"Delhi\", 22),\n",
        "        (\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. RDDs & Transformations"
      ],
      "metadata": {
        "id": "8Y6xufCrlQE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feedback RDD\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "    \"Ravi from Bangalore loved the mobile app\",\n",
        "    \"Meena from Delhi reported poor response time\",\n",
        "    \"Ajay from Pune liked the delivery speed\",\n",
        "    \"Ananya from Hyderabad had an issue with UI\",\n",
        "    \"Rohit from Mumbai gave positive feedback\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "j11ZyK23lFg2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Count total number of words\n",
        "word_count = feedback.flatMap(lambda line: line.split()).count()\n",
        "print(\"Total words:\", word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxN1y8aPlR1o",
        "outputId": "3c321b59-c16c-44de-a9dc-71c4eb038cf7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Top 3 most common words\n",
        "from collections import Counter\n",
        "words = feedback.flatMap(lambda line: line.lower().split()).collect()\n",
        "top3 = Counter(words).most_common(3)\n",
        "print(\"Top 3 common words:\", top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he-alnldlWWD",
        "outputId": "716d89fa-6d17-4b44-cee0-401e2599745e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 common words: [('from', 5), ('the', 2), ('ravi', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Remove stop words\n",
        "stop_words = {\"from\", \"with\", \"the\", \"an\", \"had\", \"and\", \"of\"}\n",
        "filtered_words = [word for word in words if word not in stop_words]"
      ],
      "metadata": {
        "id": "gsfT_fYmlYM5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Dictionary of word â†’ count\n",
        "word_dict = dict(Counter(filtered_words))\n",
        "print(\"Word frequency without stop words:\", word_dict)"
      ],
      "metadata": {
        "id": "EAfzZD5ilaA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. DataFrames Transformations"
      ],
      "metadata": {
        "id": "CBF7mkMelqqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, avg, rank, col, upper\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create DataFrame\n",
        "scores = [\n",
        "    (\"Ravi\", \"Math\", 88),\n",
        "    (\"Ananya\", \"Science\", 92),\n",
        "    (\"Kavya\", \"English\", 79),\n",
        "    (\"Ravi\", \"English\", 67),\n",
        "    (\"Neha\", \"Math\", 94),\n",
        "    (\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "u1FES72YlqAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Add grade column\n",
        "df_scores = df_scores.withColumn(\"grade\", when(col(\"score\") >= 90, \"A\")\n",
        "                                  .when((col(\"score\") >= 80), \"B\")\n",
        "                                  .when((col(\"score\") >= 70), \"C\")\n",
        "                                  .otherwise(\"D\"))"
      ],
      "metadata": {
        "id": "-M6FbyxslyXn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Average score per subject\n",
        "df_scores.groupBy(\"subject\").agg(avg(\"score\").alias(\"avg_score\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsStPRBdl0VI",
        "outputId": "64a4392f-36dd-42dc-e609-5c2a72ce4f2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "|English|     73.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Classify subject difficulty\n",
        "df_scores = df_scores.withColumn(\"difficulty\", when(col(\"subject\").isin(\"Math\", \"Science\"), \"Difficult\").otherwise(\"Easy\"))"
      ],
      "metadata": {
        "id": "o1eCtkGnl23l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Rank students per subject\n",
        "windowSpec = Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
        "df_scores = df_scores.withColumn(\"rank\", rank().over(windowSpec))"
      ],
      "metadata": {
        "id": "hgK-1Eqnl43b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# e) UDF to format names to uppercase\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def to_upper(name):\n",
        "    return name.upper()\n",
        "\n",
        "upper_udf = udf(to_upper, StringType())\n",
        "df_scores = df_scores.withColumn(\"name_upper\", upper_udf(col(\"name\")))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGxXGUN5l7F2",
        "outputId": "8915941d-c5cb-4ec2-c424-f229bedc43ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+----------+\n",
            "|  name|subject|score|grade|difficulty|rank|name_upper|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "| Kavya|English|   79|    C|      Easy|   1|     KAVYA|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|      RAVI|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|      NEHA|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|      RAVI|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|    ANANYA|\n",
            "| Meena|Science|   85|    B| Difficult|   2|     MEENA|\n",
            "+------+-------+-----+-----+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ingest CSV & JSON Save to Parquet"
      ],
      "metadata": {
        "id": "dFlynn4imBKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load"
      ],
      "metadata": {
        "id": "A49obz6qorsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"students.csv\", \"w\") as f:\n",
        "    f.write(\"\"\"id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "hi3aKmWmmApd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.option(\"header\", True).csv(\"students.csv\")\n",
        "df_csv.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyRUg4fYm1tO",
        "outputId": "b1499af7-14b7-4f8a-9444-00fd89c8c26c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"employee_nested.json\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": 101,\n",
        "    \"name\": \"Sneha\",\n",
        "    \"address\": {\n",
        "      \"city\": \"Mumbai\",\n",
        "      \"pincode\": 400001\n",
        "    },\n",
        "    \"skills\": [\"Python\", \"Spark\"]\n",
        "  }\n",
        "]\n",
        "\"\"\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OlIRBIoEoBfS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_json = spark.read.option(\"multiline\", True).json(\"employee_nested.json\")\n",
        "df_json.printSchema()\n",
        "df_json.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH_utR0KoWlt",
        "outputId": "ac10e309-fb50-4868-81d6-eed6ea0bad7a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|address         |id |name |skills         |\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatten Json\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "df_flat = df_json.select(\n",
        "    \"id\", \"name\",\n",
        "    col(\"address.city\").alias(\"city\"),\n",
        "    col(\"address.pincode\").alias(\"pincode\"),\n",
        "    explode(\"skills\").alias(\"skill\")\n",
        ")\n",
        "df_flat.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoSn7SVSofyK",
        "outputId": "6ed3f3bb-0e41-41f1-fcec-e4c24841526d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "|id |name |city  |pincode|skill |\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai|400001 |Python|\n",
            "|101|Sneha|Mumbai|400001 |Spark |\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write to parquet\n",
        "df_csv.write.mode(\"overwrite\").parquet(\"/tmp/output/csv_output\")\n",
        "df_flat.write.mode(\"overwrite\").parquet(\"/tmp/output/json_output\")"
      ],
      "metadata": {
        "id": "ByYinhhboiKg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL Temp Views & Queries"
      ],
      "metadata": {
        "id": "uvfCwnUlo7er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temp view\n",
        "df_scores.createOrReplaceTempView(\"exam_scores\")"
      ],
      "metadata": {
        "id": "fnYhns6Yo_G-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Top scorer per subject\n",
        "spark.sql(\"\"\"\n",
        "SELECT subject, name, MAX(score) as max_score\n",
        "FROM exam_scores\n",
        "GROUP BY subject, name\n",
        "ORDER BY subject, max_score DESC\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrMJ1QjbpEA3",
        "outputId": "17d9d837-534f-464d-8a4d-6803f37810fd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+---------+\n",
            "|subject|  name|max_score|\n",
            "+-------+------+---------+\n",
            "|English| Kavya|       79|\n",
            "|English|  Ravi|       67|\n",
            "|   Math|  Neha|       94|\n",
            "|   Math|  Ravi|       88|\n",
            "|Science|Ananya|       92|\n",
            "|Science| Meena|       85|\n",
            "+-------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Count of students per grade\n",
        "spark.sql(\"SELECT grade, COUNT(*) as count FROM exam_scores GROUP BY grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeVfewKCpGX7",
        "outputId": "8d16c692-2df4-486d-ae89-73ec4f9c16d7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|grade|count|\n",
            "+-----+-----+\n",
            "|    B|    2|\n",
            "|    C|    1|\n",
            "|    A|    2|\n",
            "|    D|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Students with multiple subjects\n",
        "spark.sql(\"\"\"\n",
        "SELECT name, COUNT(DISTINCT subject) as subjects\n",
        "FROM exam_scores\n",
        "GROUP BY name\n",
        "HAVING subjects > 1\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Lrl2mDpKua",
        "outputId": "9cf07e35-488d-416d-b482-2c8e66e710b9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+\n",
            "|name|subjects|\n",
            "+----+--------+\n",
            "|Ravi|       2|\n",
            "+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Subjects with average score above 85\n",
        "spark.sql(\"\"\"\n",
        "SELECT subject, AVG(score) as avg_score\n",
        "FROM exam_scores\n",
        "GROUP BY subject\n",
        "HAVING avg_score > 85\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NR5nfu3pMwu",
        "outputId": "7a6e9a21-624c-460b-f139-d926093e9149"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join attendance"
      ],
      "metadata": {
        "id": "5FBEzjiapYJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attendance = [(\"Ravi\", 22), (\"Ananya\", 18), (\"Meena\", 25), (\"Kavya\", 19), (\"Neha\", 23)]\n",
        "df_attendance = spark.createDataFrame(attendance, [\"name\", \"days_present\"])\n",
        "\n",
        "df_joined = df_scores.join(df_attendance, on=\"name\", how=\"left\")\n",
        "\n",
        "# Downgrade grade if days_present < 20\n",
        "df_final = df_joined.withColumn(\"adjusted_grade\",\n",
        "    when(col(\"days_present\") < 20,\n",
        "         when(col(\"grade\") == \"A\", \"B\")\n",
        "        .when(col(\"grade\") == \"B\", \"C\")\n",
        "        .when(col(\"grade\") == \"C\", \"D\")\n",
        "        .otherwise(\"D\"))\n",
        "    .otherwise(col(\"grade\"))\n",
        ")\n",
        "df_final.select(\"name\", \"subject\", \"score\", \"grade\", \"days_present\", \"adjusted_grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEB4NWvVpPPy",
        "outputId": "c81aaaf7-c2c7-4d24-d4df-10416809000b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+--------------+\n",
            "|  name|subject|score|grade|days_present|adjusted_grade|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "|Ananya|Science|   92|    A|          18|             B|\n",
            "|  Ravi|   Math|   88|    B|          22|             B|\n",
            "| Kavya|English|   79|    C|          19|             D|\n",
            "|  Ravi|English|   67|    D|          22|             D|\n",
            "|  Neha|   Math|   94|    A|          23|             A|\n",
            "| Meena|Science|   85|    B|          25|             B|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Partitioned Load (Full + Incremental)"
      ],
      "metadata": {
        "id": "MwkhqeIkphFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Load\n",
        "df_scores.write.mode(\"overwrite\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "YxmgASJIpbu_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Incremental Load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"subject\", \"score\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "e96HTI95plZE"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all folders (only works in local filesystem or with Hadoop API)\n",
        "import os\n",
        "print(\"Folders in /tmp/scores:\")\n",
        "print(os.listdir(\"/tmp/scores\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzlE4p5Epntp",
        "outputId": "3314a912-f2b3-4a8e-d309-23cf8a29d36f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders in /tmp/scores:\n",
            "['._SUCCESS.crc', 'subject=Science', 'subject=English', '_SUCCESS', 'subject=Math']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only Math partition\n",
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivgufWq9ppVi",
        "outputId": "1ffa0ec8-aa9f-453c-f1c9-28b96aeaec94"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----------+----+----------+\n",
            "| name|score|grade|difficulty|rank|name_upper|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "| Neha|   94|    A| Difficult|   1|      NEHA|\n",
            "| Ravi|   88|    B| Difficult|   2|      RAVI|\n",
            "|Meena|   93| NULL|      NULL|NULL|      NULL|\n",
            "+-----+-----+-----+----------+----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "Iuy2_xkvpsgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"raw_employees.csv\", \"w\") as f:\n",
        "    f.write(\"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "RLyzPslfpsHR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load raw CSV with header\n",
        "df_raw = spark.read.option(\"header\", True).csv(\"raw_employees.csv\")"
      ],
      "metadata": {
        "id": "jk_izcVnqG_i"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing bonus with 2000\n",
        "df_clean = df_raw.fillna({\"bonus\": \"2000\"})"
      ],
      "metadata": {
        "id": "UsBeGc_BqJzI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert bonus & salary to IntegerType\n",
        "from pyspark.sql.types import IntegerType\n",
        "df_clean = df_clean.withColumn(\"salary\", col(\"salary\").cast(IntegerType()))\n",
        "df_clean = df_clean.withColumn(\"bonus\", col(\"bonus\").cast(IntegerType()))"
      ],
      "metadata": {
        "id": "wWrHQFOSqLZi"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total_ctc\n",
        "df_clean = df_clean.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus\"))"
      ],
      "metadata": {
        "id": "cwogmH7XqNI8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter where total_ctc > 60000\n",
        "df_filtered = df_clean.filter(col(\"total_ctc\") > 60000)"
      ],
      "metadata": {
        "id": "VE7WHpLKqOxt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Parquet and JSON\n",
        "df_filtered.write.mode(\"overwrite\").parquet(\"/tmp/final_etl/parquet\")\n",
        "df_filtered.write.mode(\"overwrite\").json(\"/tmp/final_etl/json\")"
      ],
      "metadata": {
        "id": "Ygoks2kvqQeY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h64GT7-GqXVi",
        "outputId": "19dca302-801b-47b9-f3a1-fcbc832c1938"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Gz2ReqEfq6iO",
        "outputId": "acefb5c6-3955-44ef-9f3c-2bef95cd2634"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_df172f7e-0cdf-49e8-a087-354c7a823ff7\", \"employee_nested.json\", 155)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}